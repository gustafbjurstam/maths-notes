\documentclass[12pt]{article}

% Load AMS packages for advanced math
\usepackage{amsmath,amssymb,amsthm,mathrsfs, amsfonts}
\usepackage{pgfplots}
\usepackage{enumitem}
\pgfplotsset{compat=newest}

% Load geometry package and set margins
\usepackage[margin=1in]{geometry}

\newtheorem{lemma}{Lemma}[section]
\newtheorem{sats}{Sats}[section]
\newtheorem{theorem}{Theorem}[section]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem*{remark}{Remark}

% Shortcuts
\newcommand{\N}{\mathbb{N}}      % natural numbers
\newcommand{\real}{\mathbb{R}}   % real numbers
\newcommand{\rat}{\mathbb{Q}}     % rationals
\newcommand{\eps}{\varepsilon}    % for nice epsilon
\newcommand{\sol}{ \noindent\textbf{Solution: } }   % creates Solution:
\newcommand{\prob}[1]{ \noindent\textbf{Problem #1.} }
\newcommand{\C}{\mathbb{C}}    % complex numbers
\newcommand{\Fo}{\mathscr{F}}    % fourier
\newcommand{\F}[1]{\Fo\left[#1\right]}
\newcommand{\Fi}[1]{\Fo^{-1}\left[#1\right]}
\newcommand{\abs}[1]{\left|#1\right|}
\DeclareMathOperator{\cond}{cond}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\domain}{domain}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\prox}{prox}
\DeclareMathOperator{\TV}{TV}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\inner[2]{\left\langle#1, #2\right\rangle}


\renewcommand\part[1]{\vspace{.10in}\textbf{(#1)}}
\newcommand{\algebra}{\mathscr{A}}
\renewcommand{\L}{\mathscr{L}}
%\newtheorem*{solution}{Solution}
\newenvironment{solution}{\renewcommand{\proofname}{Lösning}\begin{proof}}{\end{proof}}
\newenvironment{bevis}{\renewcommand{\proofname}{Bevis}\begin{proof}}{\end{proof}}
\newcommand{\exempel}[1]{ \noindent\textbf{Exempel #1.} }
\usepackage{parskip}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

% Set linespacing to 1.5
\baselineskip 1.5em

\begin{center}
\textbf{\large Notes from lectures SF2529} \\
Gustaf Bjurstam\\
bjurstam@kth.se\\
\end{center}
Lecturer: Ozan Öktem
\section{Lecture 1 (2025-08-25)}
Old exams will be very different from what we will be discussing.

We will only be dealing with linear problems.

The lectures are mathematical in nature, and there are homeworks and computer labs.

We will pick problems where the straightforward simulation is simple, but tricky to reverse engineer, this is the \textit{Inverse Problem}.

This course is in a functional analytic setting. Ozan will try to reduce things to finite dimensions, but this is not always possible.

\textbf{Agenda}
\begin{itemize}
    \item Examples of inverse problems
    \item Mathematical formalisation of what an inverse problem is
    \item Naïve reconstruction (methods to solve the inverse problem) \textcolor{red}{wasn't time for}
    \item \textit{Inverse crimes} \textcolor{red}{wasn't time for}
\end{itemize}

\subsection{Examples}
The direct problem is generating data that replicates a system, aka a simulation. In the inverse problem, we seek the cause of existing data. We are reconstructing the input that caused the output.

Calibrating a model to data, is a common example of an inverse problem.

Signal processing is full of inverse problems, example, recover the analogue signal from the digital signal observed. Similarly, denoising a signal is an inverse problem.

Imaging is also quite full of inverse problems.

Training a neural network is also an inverse problem.

Decrypting a message is an inverse problem as well.

Since the 1950s there has been a theory built up which works for general inverse problems, originally people only used methods engineered for specific problems.

We have two problems as model cases. Deconvolution, and tomographic imaging.

\subsubsection{Deconvolution}
We are looking to inverse the effect of convolution, often used to model the degradation of signals. $x*k(t)= \int_{\real^d} x(\tau)k(t-\tau) \,d\tau $. We are thinking of the input as a function. The signal is a function $x:\real^d\to\real$, the data $y:\real^d\to\real$. We are observing $y$ and want to recover $x$ and the kernel $k$ is known. The kernel is $k$. The kernel is the system response. Any translation invariant, linear problem, is a convolution.

For convolution, outside the sampling interval, we either use a \textit{circular model}, meaning that $x$ is extended periodically. The other option is zero-padding.

The circular model gives $\psi_j= \sum_{i=1}^{n-1} x_jk_{(j-i)_n}$, meaning modulo $n$. Nice for using Fourier transforms/FFT.

In zero-padding, it means that $k_j=0$ if $j\neq 0,1,\dots,n-1$. This is a banded matrix.

Maybe this problem is a bit too simple.
\subsubsection{X-ray computer tomography}
All the labs will be on tomography.

We want to recover an image of the interior of an object, in ultra sounds this would correspond to recording the signal that goes through the body (not what reflects). 

Probe an object from different directions with a particle or wave, then you hope to recover the interior. 

Radiative transport equation models the interactions between particles used for probing and the object.
\begin{equation*}
    too messy
\end{equation*}

In clinical settings we don't need the full model, so we will use that instead. Beer-Lambert's law is the simplest variant of radioative transport equation and disregards many phenomena which could really happen. Assumes material is homogeneous.We call the intensity $I$, the law says $\Delta I/I_{in} = - x\Delta t$, where $\Delta t$ is the thinkness and $x$ is a material constant. What happens if $t\to 0$? In continuous setting
\begin{equation*}
    I'(t) = -xI(t)
\end{equation*}
and thus $I= I_0e^{-xt}$. Now consider two slabs with different material, $x$ is called linear attenuation coefficients. Claim $I(t+\Delta t_1+\Delta t_2) = I_0e^{-x_1\Delta t_1 - x_2 \Delta t_2}$. In a general situation with continuous differences we have $x:\real^2\to\real$, we obtain $I_{out} = I_{in}\exp(-\int x \, dt)$ (line integral). We want to find the function $x$ in the inverse problem.

More formally, we want to find $x:\real^d\to\real$ ($d=$ 2 or 3), based on information measured $I(s\omega+t\omega^\perp)$ at $s\omega+t\omega^\perp$. Here $\omega$ is just a direction, i.e. a unit vector in $\real^d$. This is a description of coordinates on lines. The difficulty here is that what we want is in $\real^d$, but the data is from lines, i.e. one-dimensional.

We have $y(\omega,s) = \int_\real x(s\omega+t\omega^\perp) \, dt$. The description of which lines we have is called acquisition geometry. Notice that the integral is a linear operator, so in the discretised version we get a matrix. 

\subsection{Mathematical formalisation}
We want to recover some signal $x\in X$, from data $y\in Y$ where $y=A(x)+\eps$, where $A:X\to Y$ is the forward operator which models how data is generated from the signal without noise, $\eps\in Y$ models noise. In this course in particular, $A$ will be a linear operator. We will also assume that we know a useful bound for the error $\norm{\eps}\leq \delta$. We ignore the statistical characteristic of $\eps$.

We also assume that $A$ to be a linear, bounded, operator.

\newpage
\section{Lecture 2 (2025-08-26)}

\textbf{From last time}
\begin{itemize}
    \item Naïve reconstruction (methods to solve the inverse problem)
    \item \textit{Inverse crimes}
\end{itemize}

\textbf{For today}
\begin{itemize}
    \item Hilbert spaces
    \item Compact operators
    \item Moore-Penrose inverse (pseudoinverse) \textcolor{red}{didn't have time for}
    \item Spectral theorem and SVD \textcolor{red}{mentioned SVD?}
\end{itemize}

\subsection{Hadamard conditions}
\begin{enumerate}[label=H\arabic*]
    \item There should be at least one solution (existence)
    \item There should be at most one solution (uniqueness)
    \item The solution should be depend continuously on data (stability)
\end{enumerate}
A problem where all three conditions hold is called \textit{well posed}. If any fail then it is \textit{ill-posed}.

Unfortunately most inverse problems are ill-posed. By changing the notion of solution we can often handle conditions 1 and 2, so stability is usually the bigger problem.

\subsection{Naïve reconstruction}
Assume $A:X\to Y$ injective, with  $A^{-1}:A(X)\to X$. If $y\notin A(X)$ (due to $\eps$), then there is no $x\in X$ such that $y=A(x)$. 

If $y\notin A(X)$, project $y\mapsto \tilde{y}\in A(x)$. We than have two naive inversions
\begin{enumerate}
    \item $x^*=A^{-1}\tilde{y}$
    \item If $A$ is not invertible, use least-squares, solving $x^* = \argmin_x \norm{A(x)-y}_y^2$ or $x^*= \argmin(\norm{x})$ such that $x$ is a solution to $A(x)=\tilde{y}$.
\end{enumerate}

In order to invert the convolution we use Fourier transform. $\F{y}=\F{x}\F{k}$. We can then just take $x = \Fi{\frac{\F{y}}{\F{k}}}$. This clearly doesn't work if $\F{k}$ ever is zero, the division is the problem.

For the tomography problem in 2D, we have (harmonic analysis needed) an inversion formula. $t\in\real^2$

\begin{equation*}
    x(t) = A^{-1}(y)(t) = \frac{1}{2\pi}\int_0^\pi G(y)(\theta,\underbrace{t\cdot\omega(\theta)}_p) \, d\theta
\end{equation*}
where 

$$G(y)(\theta,p)= \frac{1}{2\pi}\int_\real \F{y(\theta,\cdot)}(\zeta)e^{i\zeta \cdot p}|\zeta|\, d\zeta = \Fi{\F{y(\theta,\cdot)}(\zeta) |\zeta|}.$$.

\subsection{Inverse crimes}
\begin{itemize}
    \item Testing methods on noise-free data
    \item If you insist on noise-free data, you should not use the same sampling frequency in the for $m$ and $n$ (data is in $\real^{m}$ and cause in $\real^n$).
\end{itemize}


\subsection{Can we work in finite dimensions?}
Actual measured data is given on finitely many points. There is a sampling operator $S_y:Y\to\real^m$. In the \textit{semi-discrete inverse problem} we keep $x$ continuous but use sampled data. $\bar{y} = (S_y\circ A)(x) + \bar{\eps}$. There is not much to do here however, we should go fully discrete or fully continuous.

We need $E_x:\real^n\to X$ to extend the finite $x$ to actual functions. We want $(S_y\circ A \circ E_x)= \mathcal{A}$. If all of the operators are linear, then the total transformation is also linear. 

In the fully discrete problem, we want to find $\bar{x}\in\real^n$ such that $y=(S_y\circ A\circ E_x)x + \bar{e}$. The matrix we obtain for $\mathcal{A}$ quickly gets obscenely large, and this is a huge problem in applications that we will need to handle. It is often not possible to store the entire matrix in memory.


\subsection{Singular Value Decomposition}
Any matrix $A\in\real^{m\times n}$ has a singular value decomposition $A=UDV^*$ where $D$ is diagonal with positive, sorted falling, values on the diagonal and $U,V$ are unitary. The condition number is given as the ratio of the largest and smallest singular value.

Problem with H3 in discrete setting is that even though the condition number is large, the discrete problem is still very much continuous. If $\cond A\to \infty$ as $n,m\to\infty$ that is a problem, as we do not get more accuracy with better sampling.

\newpage

\section{Lecture 3 (2025-09-01)}
\textbf{For today}
\begin{itemize}
    \item Functional analysis on real vector spaces
    \item Moore-Penrose inverse
    \item Compact operators
\end{itemize}

\subsection{Functional analysis}
\begin{definition}
    A \textit{normed linear space} is a vector space $X$ over $\real$, together with a \textit{norm} $\norm{\cdot}:X\to\real^{\geq0}$, such that 
    \begin{enumerate}[label=(\alph*)]
        \item $\norm{\alpha x}=|\alpha|\norm{x}$ for all $\alpha\in \real$ and $x\in X$,
        \item $\norm{x_1+x_2}\leq \norm{x_1}+\norm{x_2}$, and
        \item $\norm{x}=0\Longleftrightarrow x=0$. 
    \end{enumerate}
\end{definition}

\begin{definition}
    A normed linear space where every Cauchy sequence converges is called a \textit{Banach space}. 
\end{definition}

\begin{definition}
    Let $X$ is a vector space. An \textit{inner product} is a binary map $\langle\cdot,\cdot\rangle: X\times X \to \real$ such that
    \begin{enumerate}[label=(\alph*)]
        \item $\langle x_1,x_2\rangle = \langle x_2,x_1\rangle$,
        \item $\langle\alpha x_1,x_2\rangle = \alpha\langle x_1,x_2\rangle$,
        \item $\langle x_1+x_2,x_3\rangle = \langle x_1,x_3\rangle+\langle x_2,x_3\rangle$, and
        \item $\langle x,x\rangle \geq 0$, with equality only for $x=0$.
    \end{enumerate}
\end{definition}

\begin{definition}
    A \textit{Hilbert space} is a Banach space where the norm is defined by an inner product $\norm{x} = \sqrt{\inner{x}{x}}$.
\end{definition}

\subsection{Moore-Penrose Inverse}
If H1 or H2 fails to hold, we change what we mean by solution. Often we go for the minimum norm solution. The Moore-Penrose inverse is a way of obtaining the minimum norm solution.

\begin{definition}
    \begin{enumerate}[label=(\alph*)]
        \item $x^\dagger \in X$ is a least square solution to an inverse problem, if $x^\dagger \in \argmin_{x\in X} \norm{Ax-y}_Y$
        \item $x^\dagger \in X$ is a minimum norm solution to the inverse problem if 
        \begin{equation*}
            x^\dagger =\begin{cases}
                \argmin_{x\in X} \norm{x}_X,\\
                x \text{ is a least square solution}.
            \end{cases}
        \end{equation*}
    \end{enumerate}
\end{definition}

Assume $\range(A)$ is not closed, then $X\mapsto \norm{Ax-y}$ does not attain a minimum. Thus, the least square solution does not exist.  It is natural to ask for what problems a least square solution exist.

\begin{theorem}[Moore-Penrose]
    Assume $A\in \mathcal{L}(X,Y)$, and set $\tilde{A}:\ker(A)^\perp \to A(X)$ as $\tilde{A}=A\Big|_{\ker(A)^\perp}$, then there exists a \textbf{unique} extension of $A^\dagger $ of $\tilde{A}^{-1}$ where the domain of $\domain(A^\dagger )=A(X)\oplus A(X)^\perp$ and $\ker A^\dagger  = A(X)^\perp$.
\end{theorem}

There are three ways to define the Moore-Penrose inverse on a Banach space.

\begin{theorem}
    If $y\in\domain(A^\dagger )$, then $x$ is a least square solution if and only if $A^*Ax=A^*y$.
\end{theorem}

The condition H3 fails if $A^\dagger $ does not exist or if $A^\dagger $ is not continuous.

If $A$ is a linear map, then H1-H3 are not independent. 
\begin{theorem}
    If $X,Y$ are Banach spaces and $A\in\mathcal{L}(X,Y)$. Then H1+H2 $\implies$ H3.
\end{theorem}
\begin{proof}
    Any linear map between normed spaces that is bounded, is continuous. Thus it is enough to show that $A^{-1}$ is linear and bounded. \textcolor{red}{Do this perhaps}
\end{proof}
\textcolor{red}{What is the graph of an operator???}

\subsection{Compact operators}
Compact operators are the infinite dimensional analogue to ill-conditioned matrices.

\begin{definition}
    Let $X,Y$ are Banach spaces. The operator $A\in\mathcal{L}(X,Y)$ is \textit{compact} if it maps bounded sequences in $X$ to sequences in $Y$ which has a convergent subsequence.
\end{definition}

\textcolor{blue}{Think about how Theorem 3.3 and 3.4 go together.}

\begin{theorem}
    If $A\in \mathcal{L}(X,Y)$, compact and $\dim(A(X))=\infty$, then $A^\dagger $ is not continuous.
\end{theorem}
\begin{remark}
    Equivalently, the preimage of $A$ is a compact set.
\end{remark}

Examples of compact operators are 
\begin{itemize}
    \item if $A\in\mathcal{L}(X,Y)$ has finite rank, then $A$ is compact.
    \item The identity mapping $I:X\to X$ is compact if and only if $X$ is finite dimensional.
    \item If $A\in \mathcal{L}(X,Y)$ and $B:Y\to E$ is compact, then $B\circ A$ is compact.
    \item If we have Hilbert spaces, $A$ is compact if and only if $A^*$ is compact.
\end{itemize}
\textcolor{red}{Prove the statement about the identity mapping}

\begin{theorem}
    Let $A\in \mathcal{L}(X,Y)$ be compact. If $A(X)$ is infinite dimensional, then $A^\dagger $ is not continuous, so H3 does not hold.
\end{theorem}
\begin{proof}
    $A$ is injective $\implies A^\dagger =A^{-1}$. Assume $\dim A(X)=\infty$ and that $A^{-1}\in \mathcal{L}(X,Y)$. Then the identity mapping $I=A^{-1}\circ A$, is continuous and compact. Then $X$ is finite dimensional, contradicting $\dim A(X)=\infty$. 
\end{proof}

\begin{theorem}
    If $A_n$ is a sequence of compact operators converging to $A$, then $A$ is compact.
\end{theorem}

Example: $X=L^2(\Omega)$, $A:X\to X$ defined by $Ax=\int_\Omega k(s,t)x(t) \,dt$ with $k\in L^2(\Omega\times \Omega)$, then $A$ is compact.

Proof: \textbf{Fourier analysis pog}. The space $L^2(\Omega)$ has an orthonormal basis $\{\varphi_i\}$. So does $L^2(\Omega\times \Omega)$, set $\{\phi_i(s)\phi_j(t)\}$. We can write $k = \sum_{i,j} k_{i,j}\phi_i\phi_j$, where $k_{i,j}$ are the Fourier coefficients $k_{i,j} = \int_{\Omega^2}k\phi_i\phi_j$. Equalities are almost everywhere. Set $k_n = \sum_{i,j=0}^n k_{i,j}\phi_i\phi_j$ and $A_n(x) =\int_\Omega k_n(s,t)x(t)\,dt $. If each $A_n$ is compact, and if $A_n\to A$, then $A$ is compact. Notice that $\dim A_n(X)$ is finite, \textcolor{red}{......}

Let $X_n=A_n(X)$, so $X_n$ has finite dimension. 

this is theorem 3.4 in the book

\begin{equation*}
    \begin{aligned}
    \norm{(A-A_n)x}^2_{X_n} &= \norm{\int_{\Omega} (k-k_n)(s,t)x(t)\, dt}_{X_n}^2\\
                            &= \int_{\Omega} \left|  \int_{\Omega} (k-k_n)(s,t)x(t)\, dt \right|^2 \,ds\\
                            &\leq \int_\Omega \left( \int_\Omega|k-k_n|x\,dt \right)^2 \,ds\\
                            &\leq 
    \end{aligned}
\end{equation*}

\begin{definition}
    On $\mathcal{L}(X,Y)$ we define the norm by $\norm{A} = \sup_{\norm{x}_X=1}\norm{Ax}_Y$.
\end{definition}

\newpage
\section{Lecture 4 (2025-09-04)}

\textbf{Today}
\begin{itemize}
    \item Singular Value Decomposition of compact operator
    \item Formal definition of regularisation method
    \item Spectral regularisation
    \begin{itemize}
        \item Truncated SVD
        \item Tikhonov regularisation
    \end{itemize}
\end{itemize}

\subsection{SVD of operators}
Operators might not have countably many singular values. Compact operators however, luckily, have countably many eigenvalues values.

\begin{theorem}
    Let $X,Y$ be Banach spaces. If $A\in\mathcal{L}(X,Y)$ is compact. Then $\dim(A(X)) =\infty$ and $A^\dagger :Y\to X$ is not continuous. And $A^\dagger \Longleftrightarrow A(X)$ is closed. 
\end{theorem}

\begin{theorem}[Spectral decomposition]
    Let $X$ be a Hilber space, $N:X\to X$ is self-adjoint and compact. Then there exists an ON-basis $\{\phi_j\}\subset X$ of $\overline{N(X)}$ and $\{\lambda_j\}\subset \real$ with $\abs{\lambda_1}\geq \abs{\lambda_2} \geq \dots > 0$ such that $N(x) = \sum_j \lambda_j \inner{x}{\phi_j}\phi_j$ for all $x\in X$.
\end{theorem}

\begin{theorem}[SVD of compact operator]
    Let $X,Y$ be Hilbert spaces, and $A\in \mathcal{L}(X,Y)$ be compact. Then there exists:
    \begin{enumerate}[label=(\alph*)]
        \item $\{\sigma_j\} \subset \real$ s.t. $\sigma_1\geq \sigma_2\geq\dots\geq 0$,
        \item $\{\phi_j\}\subset X$ is an ON-basis of $\ker A^\perp$, and
        \item $\{\psi_j\}\subset Y$ is an ON-basis of $\overline{A(X)}$.
    \end{enumerate}
    such that 
    \begin{enumerate}[label=(\roman*)]
        \item $A(\phi_j) = \sigma_j \psi_j$, and $A*(\psi_j) = \sigma_j \phi_j$,
        \item $A(x) = \sum_j \sigma_j \inner{x}{\phi_j}_X \psi_j$ for all $x\in X$.
        \item $A^*(y) = \sum_j \sigma_j \inner{y}{\psi_j}_Y \phi_j$ for all $y\in Y$.
    \end{enumerate}
\end{theorem}


Notice that the eigenvalues of $N=A^*A$ with eigenvectors $\{\phi_j\}$ are also eigenvalues of $AA^*$ with eigenvectors $\{ \psi_j\}$, we have $\sigma_j = \sqrt{\lambda_j}$ and $\psi_j = \frac{1}{\sigma_j}A\phi_j$.

\begin{theorem}[Picard criteria]
    Let $A\in \mathcal{L}(X,Y)$ be compact, with SVD $\{(\sigma_j,\phi_j, \psi_j)\}$ and $y\in \overline{A(X)}$, then
    \begin{equation*}
        y\in A(X) \Longleftrightarrow \sigma_j \frac{\abs{\inner{y}{\psi_j}_Y}^2}{\sigma_j^2}<\infty.
    \end{equation*}
\end{theorem}

\begin{theorem}[SVD of $A^\dagger $]
    If $A\in \mathcal{L}(X,Y)$ compact with SVD $\{(\sigma_j,\phi_j, \psi_j)\}$  and $y\in\domain A^\dagger $, then
    \begin{equation*}
        A^\dagger y = \sum_j \frac{1}{\sigma_j}\inner{y}{\psi_j}_Y\phi_j.
    \end{equation*}
\end{theorem}

Notice that the right-hand side accepts any $y\in Y$, could we extend $A^\dagger $ this way?
\begin{itemize}
    \item The SVD of $A^\dagger $ encodes the stability properties of "inverting" $A$.
    \item Consider $y^\delta = y +\eps$, where $y$ is the ideal data and we take $\eps = \delta \psi_j$. The minimum norm solutions $x^\dagger  = A^\dagger y$, and $x^\delta = A^\dagger y^\delta$. The difference becomes $\frac{\delta}{\sigma_j}\phi_j$. Which for small $\sigma_j$ might grow very large.
    \item We can use the singular values to put a "measure" on ill-posedness.
    \begin{itemize}
        \item We can say that a problem is severely ill-posed if the singular values decay exponentially.
        \item Moderately ill-posed if it is not severely ill-posed.
    \end{itemize}
\end{itemize}

\subsection{Regularisation method}
Earlier we changed the solution to get out of the problems of H1 and H2, can we do it again to solve H3?

The first idea is to truncate the SVD, and look in a finite dimensional sub-space.

The second is \textit{filter functions} where we instead apply functions to the singular values first.

\begin{definition}
    Let $X, Y$ be Banach spaces and $A\in \mathcal{L}(X,Y)$. A family $\{R_\alpha\}$ is a regularisation of $A^\dagger :Y\to X$ if 
    \begin{enumerate}[label=(\alph*)]
        \item each $R_\alpha:Y\to X$,
        \item each $R_\alpha$ is continuous
        \item $\lim_{\alpha\to 0} R_\alpha(y) = A^\dagger (y)$ for all $y\in \domain A^\dagger $.
    \end{enumerate}
    If each $R_\alpha\in\mathcal{L}(X,Y)$, then the regularisation is said to be linear.
\end{definition}

Hence, we have reformulated the inverse problem by looking for solutions to $R_\alpha$ and thus ensured H3.

\begin{theorem}
    Let $X,Y$ be Hilbert spaces, $A\in \mathcal{L}(X,Y)$, and $\{R_\alpha\}$ is a linear regularisation. If 
    \begin{enumerate}[label=(\alph*)]
        \item $A^\dagger $ is not continuous, then $\{ R_{\alpha} \}$ is not uniformly bounded. Then there exists $y\in Y$ such that $\norm{R_{\alpha} y}_X \to \infty$ as $\alpha \to 0$. 
        \item $\norm{AR_\alpha}_{\mathcal{L}(X,Y)} < \infty \implies \norm{R_\alpha y}_X \to \infty$ as $\alpha \to 0$ for all $y \notin \domain A^\dagger $. 
    \end{enumerate}
\end{theorem}

In applications, we cannot expect $y\in \domain A^\dagger $. Consider data $y^\delta = y+ \eps$ with  $\norm{y^\delta - y}\leq delta$. And a linear regularisation $R_\alpha$, the total error $\norm{R_\alpha(y^\delta)-A^\dagger (y)}_X \leq \norm{R_\alpha(\eps)}_X + \norm{R_\alpha y - A^\dagger  y}_X \leq \delta \norm{R_\alpha}_{\mathcal{L}(X,Y)} + \norm{R_\alpha y - A^\dagger  y}_X$. First term is \textit{data error} and second \textit{approximation error}.

The data error does not stay bounded as $\alpha \to 0$. The approximation error goes to 0 as $\alpha\to 0$. Picking a good $\alpha$ is almost like the bias-variance trade-off.

\subsection{Spectral regularisation}
Tikhonov was the first to study regularisation that wasn't built problem specific, that it generally applicable. 

\begin{definition}[Spectral regularisation]
    Let $X,Y$ be Hilbert spaces and $A\in \mathcal{L}(X,Y)$, so $A^\dagger $ is not continuous, and $A$ have SVD $\{(\sigma_j,\phi_j, \psi_j)\}$. The SVD of $A^\dagger $ is $\{(1/\sigma_j,\psi_j, \phi_j)\}$. Define $R_\alpha:Y\to X$ as 
    \begin{equation*}
        R_\alpha(y) = \sum_j g_\alpha(\sigma_j)\inner{y}{\psi_j}\phi_j, \quad \forall y\in Y,
    \end{equation*} 
    where $g_\alpha:\real^{+} \to \real^+$ is called a \textit{spectral filter} and 
    \begin{enumerate}[label=(\alph*)]
        \item $g_\alpha(\sigma)\to \frac{1}{\sigma}$ as $\alpha\to 0$,
        \item $g_\alpha \leq C_\alpha$ for all $\sigma >0$.
    \end{enumerate}
    The first condition ensures convergence to $A^\dagger $ and the second gives continuity of $R_\alpha$.
\end{definition}

\begin{definition}
    \textit{Truncated SVD} is a special case of spectral regularisation, given by choosing \begin{equation*}
        g_\alpha(\sigma) = \begin{cases}
            \frac{1}{\sigma}, \quad \sigma\geq \alpha\\
            0, \quad \text{else}.
        \end{cases}
    \end{equation*}
\end{definition}

\begin{definition}
    In \textit{Tikhonov regularisation} we take $g_\alpha(\sigma) = \frac{\sigma}{\sigma^2 + \alpha}$.
\end{definition}

In large scale problems it becomes impossible to compute the SVD, this causes big problems.

\newpage

\section{Lecture 5 (2025-09-08) CANCELLED}
Ozan was sick :c

\newpage

\section{Lecture 6 (2025-09-11)}
\textbf{Today}
\begin{itemize}
    \item Parameter choice rules
    \item Convergence rates / stability estimates
    \item A priori parameter choice rules
    \item A posteriori parameter choice rules
    \item Spectral regularisation
\end{itemize}

\subsection{Parameter choice rules}
\begin{definition}
    A \textit{parameter choice rule} is a formally a mapping $\alpha:\real^+\times Y\to \real^+$. We call $\alpha$
    \begin{enumerate}[label =(\roman*)]
        \item \textit{a priori} if it only depends on $\delta$,
        \item \textit{a posteriori} if it depends on both $\delta$ and $y^\delta$,
        \item \textit{heuristic} if it only depends on $y^\delta$.
    \end{enumerate}
\end{definition}

\begin{definition}
    A regularisation $\{R_\alpha\}$ of $A^\dagger $ is convergent with parameter choice rule $\alpha:\real^+\times Y\to \real^+$ if 
    \begin{equation*}
        \lim_{\delta\to 0} \sup_{u\in Y : \norm{y-u}\leq \delta} \norm{R_{\alpha(\delta,u)}(u) - A^\dagger (y)}_X=0
    \end{equation*} 
    hold for any $y\in \domain A^\dagger $.
\end{definition}
\begin{remark}
    Each $u\in Y$ such that $\norm{u-y}_Y\leq \delta$ can be written as $u=y+\eps$ with $\norm{\eps}_Y\leq \delta$.
\end{remark}

Can we determine "forms" of choice rules?

\subsection{Convergence rates / stability estimates}
\begin{definition}
    Assume a regularisation $\{R_\alpha\}$ which is convergent with parameter choice rule $\alpha$. Assume $y^\delta = y + \eps$ with $\norm{\eps}\leq \delta$. A \textit{stability estimate} is a function $C:\real^+\to\real^+$ such that 
    \begin{equation*}
        \norm{R_{\alpha(\delta,y^\delta)}(y^\delta) - A^\dagger (y)}_X \leq C(\delta)
    \end{equation*}
    where $C$ is increasing and vanishing at 0.
\end{definition}

\subsection{A priori choice rules}
\begin{theorem}
    Let $\{R_\alpha\}$ be a regularisation method for $A^\dagger $. Then there exists an a priori choice rule $\alpha:\real^+\to\real^+$ such that $\{R_\alpha\}$ is convergent with $\alpha$.
\end{theorem}

\begin{theorem}
    Let $\{R_\alpha\}$ be a linear regularisation method for $A^\dagger $. It is convergent with respect to an a priori choice rule $\alpha$ if and only if 
    \begin{enumerate}[label=(\alph*)]
        \item $\lim_{\delta\to 0} \alpha(\delta) = 0$, and 
        \item $\lim_{\delta\to 0} \delta\norm{R_\alpha}_{\mathcal{L}(X,Y)} = 0$.
    \end{enumerate}
\end{theorem}

Often people choose $\alpha = \delta^p$ for some $p\in(0,1)$. We define the \textit{best} $p$ as the one which achieves the fastest convergence rate. Finding the best $p$ depends on the regularisation method and requires additional information about $x^\dagger =A^\dagger y$.

\subsection{A posteriori choice rules}
We will only study one, the most widely used rule. For notation's sake, let $x_{\alpha,\delta} = R_\alpha y^\delta$, and $y= A^\dagger x^\dagger $, where $x^\dagger $ is minimum norm least square solution. The idea is to chose $\alpha(\delta,y^\delta)$ such that 
\begin{equation*}
    \norm{A(x_{\alpha(\delta,y^\delta),\delta}) -y^\delta } \leq \delta.
\end{equation*}
In order to do this practically, we take a sequence of $\alpha_1>\alpha_2>\dots>0$ and compute $x_j=R_{\alpha_j} y^\delta$, and then just check which $\alpha$ is the largest acceptable. This is called the Morozov discrepancy principle.

\subsection{Spectral regularisation}
Let $A$ be a compact linear operator from $X$ to $Y$. Then $A$ has a singular value decomposition $\{(\sigma_j,\phi_j,\psi_j)\}$.

\begin{theorem}
    If $x_\alpha = R_\alpha y$, where $\{R_\alpha\}$ is Tikhonov regularisation, then 
    \begin{equation*}
        x_\alpha = \argmin_{x\in X} \norm{Ax-y}_Y^2+\alpha\norm{x}_X^2,
    \end{equation*}
    and $x_\alpha$ solves $(A^*A+\alpha I)x_\alpha = A^* y$. 
\end{theorem}
If we change the last term in the first equation we start talking about variational regularisation, which in some ways is much more powerful than spectral regularisation. The second term in some way embeds prior knowledge, and penalises unnatural behaviour.

\newpage

\section{Lecture 7 (2025-09-15)}
\textbf{Today}
\begin{itemize}
    \item Tikhonov regularisation as variational model
    \item General variational models \textcolor{red}{wasn't time for}
    \begin{itemize}
        \item Interpretation
        \item Existence and uniqueness
        \item Convergence
        \item Stability estimates
        \item Computational methods
        \item Examples
    \end{itemize}
\end{itemize}

\subsection{Tikhonov regularisation as a variational model}
A variational model means that we define the model as the solution to an optimisation problem.

\textbf{Advantages and disadvantages of definition via SVD}

\begin{itemize}[label=($\boldsymbol{+}$)]
    \item Special case of spectral regularisation, that can be theoretically analysed.
\end{itemize}
\begin{itemize}[label=$(\boldsymbol{-})$]
    \item The SVD is not always computationally unfeasible.
    \item Requires $A$ to be linear and compact, it would be nice to be able to apply the method to a wider class of problems.
    \item Regularising property only operates by filtering singular values. Hard to encode other prior knowledge.
\end{itemize}

We address these problems through the variational formulation of Tikhonov regularisation.

First we should prove Theorem 6.3.

\begin{proof}[Proof of Theorem 6.3]
    First prove $x_\alpha=R_\alpha\implies (A^*A+\alpha I)x_\alpha = A^*y$. Notice $$(\alpha I)x_\alpha = \sum \alpha \frac{\sigma}{\sigma^2+\alpha}\inner{y}{\psi}\varphi,$$ and 
    \begin{equation*}
        \begin{aligned}
            (A^*A)x_\alpha &= (A^*A)\sum \frac{\sigma}{\sigma^2+\alpha}\inner{y}{\psi}\varphi\\
            &= \sum \frac{\sigma}{\sigma^2+\alpha}\inner{y}{\psi} (A^*A)\varphi\\
            &= \sum \frac{\sigma}{\sigma^2+\alpha}\inner{y}{\psi} \sigma^2\varphi
        \end{aligned}
    \end{equation*}
    Where we are allowed take $(A^*A)$ inside due to convergence and continuity. Thus
    \begin{equation*}
        (A^*A +\alpha I)x_\alpha = \sum {\sigma}\inner{y}{\psi}\varphi = A^*y.
    \end{equation*}
    Now for the other direction. Assume $x\in X$ solves $(A^*A + \alpha I)x = A^*y$, show that $x=x_\alpha = R_\alpha y$ for some $\alpha>0$. We know that $X = \ker A \oplus \ker A^\perp$, and $\ker A^\perp=\overline{\range A^*}$. From SVD theorem we have $\varphi_n$ is an ON-basis of $\overline{\range A^*}\subseteq X$, and that $A^*A\varphi=\sigma^2\varphi$. Since $x\in X$, we can write it as $x=x'+x''$, with $x'\in\overline{\range A^*}, x''\in \ker A$, and 
    \begin{equation*}
        x' = \sum \inner{x}{\varphi} \varphi
    \end{equation*}
    Which means that 
    \begin{equation*}
        x = \underbrace{\sum \inner{x}{\varphi} \varphi}_{x'} + \underbrace{\pi(x)}_{x''}
    \end{equation*}
    where $\pi$ is the projection onto the $\ker A$. We know that 
    \begin{equation*}
        \begin{aligned}
        (A^*A + \alpha I)(x'+x'') &= A^*Ax'+\alpha I x' + \underbrace{A^*Ax''}_{=0} + \alpha I x''\\
        &= \sum (\sigma^2 + \alpha) \inner{x}{\varphi} \varphi + \alpha x''\\
        &= \sum \sigma \inner{y}{\psi}\varphi
        \end{aligned}
    \end{equation*} 
    Since the right-hand side is in $\range A^*$, we must have $x'' = 0$, so $x=x'$.  Thus, since we can solve for the factor in front of each basis vector individually, 
    \begin{equation*}
        x'=x = \sum \frac{\sigma}{\sigma^2+\alpha} \inner{y}{\psi}\varphi
    \end{equation*}
    thus $x=x_\alpha$.
\end{proof}
\begin{remark}
    Notice we didn't prove the optimisation statement.
\end{remark}
The normal equations $A^*Ax=A^*y$ is equivalent to $x=\argmin \norm{Ax -y}_Y^2$. 
\begin{proof}[Proof of optimisation part of Theorem 6.3]
    Knowing that $x=R_\alpha y$ we have to show that $x_\alpha$ is the minimiser to $x'\mapsto \norm{Ax-y}^2_Y+\alpha\norm{x}^2_X=\mathcal{E}_\alpha(x')$. Equivalently, that if $x\in X$ then $\mathcal{E}_\alpha(x)>\mathcal{E}_\alpha(x_\alpha)$. Let $x\in X$, then 
    \begin{equation*}
        \begin{aligned}
            \mathcal{E}_\alpha(x) - \mathcal{E}_\alpha(x_\alpha) &= \inner{Ax-y}{Ax-y} + \alpha\inner{x}{x}-\inner{Ax_\alpha-y}{Ax_\alpha-y} + \alpha\inner{x_\alpha}{x_\alpha}\\
            &= \norm{Ax-Ax_\alpha}^2 + \alpha \norm{x-x_\alpha}^2 + 2\inner{\underbrace{A^*(Ax_\alpha -y) + \alpha x_\alpha}_{=0}}{x-x_\alpha}
        \end{aligned}
    \end{equation*}
    Now for the other direction. Assume $\tilde{x}$ minimises $\mathcal{E}_\alpha$. We must show $R_\alpha y = \tilde{x}$. Since $\tilde{x}$ is a minimiser, so $\mathcal{E}_\alpha(x)-\mathcal{E}_\alpha(\tilde{x})\geq0$ for any $x\in X$. In particular, $x= \tilde{x} + t x_0$, for $t>0$ and $x_0\in X$. Then
    \begin{equation*}
        0\leq \mathcal{E}_\alpha(x)-\mathcal{E}_\alpha(\tilde{x}) = t^2 \norm{Ax_0}^2 + t^2 \alpha \norm{x_0}^2 + 2t \inner{A^*(A\tilde{x}-y)+ \alpha \tilde{x}}{x_0}
    \end{equation*}
    Divide by $t>0$ and take limit as $t\to0$. Then
    \begin{equation*}
        \inner{A^*(A\tilde{x}-y)+ \alpha \tilde{x}}{x_0}\geq 0
    \end{equation*}
    for any $x_0\in X$. Since $x_0$ is arbitrary we have that the left entry of the inner product is 0. (Otherwise $-x_0$ would give negative inner product which isn't allowed). Thus $\tilde{x}=x_\alpha$. 
\end{proof}
Thus we could define Tikhonov regularisation from the optimisation problem, but using that as a starting point makes it difficult to prove that it is formally a regularisation method.

\subsection*{Variational models}
Generally we write variational models as 
\begin{equation*}
    R_\alpha(y) = \argmin \mathcal{L}_Y(Ax,y) + \alpha s(x)
\end{equation*}
where $\mathcal{L}_Y:Y\times Y\to \real^+$ measures the data fidelity and $s:X\to\overline{\real^+}$ is the regulariser. It is often used to encode prior knowledge.

\newpage
\section{Lecture 8 (2025-09-18)}

\textbf{Today}
\begin{itemize}
    \item Examples of variational models
    \item Theory of variational models
\end{itemize}
A variational model defines a mapping $R_\alpha:Y\to X$ such that 
\begin{equation*}
    R_\alpha(y) \in \argmin_{x\in X} \mathcal{E}_{\alpha(x),y}
\end{equation*}
with $\mathcal{E}_\alpha(x) = \frac{1}{2}\norm{Ax-y}^2 +\alpha S(x)$, where $S$ is the regulariser (in some sense, not rigorous) chosen to ensure $R_\alpha$ is a regularisation model, $S(x)=\frac{1}{2}\norm{x}^2$ is classical Tikhonov regularisation.

\subsection{Tikhonov-Philips Ellipse}
Assume $\mathscr{D}:X\to Z$ linear, and $S(x) = \frac{1}{2}\norm{\mathscr{D}(x)}_Z^2$. 
\begin{itemize}
    \item The Dirichlet is a special case given by $\mathscr{D}(x) = \nabla x$.
    \item Let $\mathscr{D}:H^1(\Omega) \to \mathcal{L}^2(\Omega, \real^d)$, $H^1(\Omega) = \{x\in \mathcal{L}^2:\nabla x\in \mathcal{L}^2(\omega,\real^d)\}$.
    \item Similar, the norm on $H^1$ $S(x) = \frac{1}{2} \norm{x}^2+\norm{\nabla x}^2$. Obviously you can weight this differently, introducing more parameter.
\end{itemize}

\subsection{Total variation}
The Tikhonov regularisers smooth edges, that is not always desirable. Let $X=W^{1,1}(\Omega) = \{x\in\mathcal{L}^1(\Omega): \nabla x\in\mathcal{L}^1(\Omega,\real^d)\}$ The TV (total variation) functional is a mapping
\begin{equation*}
    \TV:X\to\real^+, \qquad \TV(x) = \int_\Omega \abs{\nabla x}
\end{equation*} 
In many applications $W^{1,1}$ is too restrictive, as we need jump discontinuities. So we have to extend TV to a broader class of functions.
\begin{equation*}
    \text{BV}(\Omega)= \{x\in \mathcal{L}^1:\norm{x}_{\mathcal{L}^1}+\TV <\infty\}
\end{equation*}
Where 
\begin{equation*}
    \TV(x) = \sup_{\phi\in D(\Omega,\real^d)}\int_\Omega u(x) (\nabla\cdot \phi)(t)\,dt
\end{equation*}
and $D(\Omega,\real^d) = \{\phi\in C_c^\infty(\Omega, \real^d) : \sup_{t\in\Omega} \abs{\phi(t)}_{\real^d}<1\}$. Unfortunately then, BV is no longer a Hilbert space, but only Banach. 

\subsection*{$\ell^1$-regularisation (Lasso)}
Here $X$ is the set of sequences in $\ell^1$, and $A:\ell^1\to\ell^2$. And $S(x) = \norm{x}_{\ell^1}$. This is sparsity promoting for some reason. Only proven in 2004, compressed sensing. 

\subsection{Theory}
We have still not properly defined the mapping $R_\alpha$, as we haven't shown uniqueness of the minimiser. In variational models, we define $R_\alpha$ indirectly as the solution to a minimisation problem. Thus we have to show that the problem has a solution, and preferably that it is unique. 

We also need to show that $y\mapsto R_\alpha(y)$ is a continuous mapping, and that $R_\alpha(y) \to x^\dagger $ as $\alpha\to 0$. If all of this is done, then we can actually call it a regularisation method.

The first step is quite technically demanding. So we will be trying to dumb it down. Normally this is done in Banach spaces, we try a slightly easier version in Hilbert spaces.

\textbf{Today and next time most technically demanding lectures of the course!!!!}

\textbf{Basic assumptions (Hilbert setting)}
\begin{enumerate}[label=(\alph*)]
    \item $A:X\to Y$ continuous
    \item $S:X\to\overline{\real}$ is \textit{proper} and weakly \textit{lower semi-continuous} (l.s.c.)
    \item $S$ is \textit{coercive}
\end{enumerate}
\begin{remark}
    If $S$ is continuous and convex, then it is weakly l.s.c.
\end{remark}

\begin{definition}
    A function $S:X\to\overline{\real}$ 
    \begin{enumerate}[label={(\alph*)}]
        \item is proper if $S(x)>-\infty$ and $\domain S\neq \emptyset$.
        \item is weakly l.s.c. at $x_0\in X$ if
        \begin{equation*}
            \liminf_{n\to\infty} S(x_n) \geq S(x_0)
        \end{equation*}
        whenever $x_n$ weakly converges to $x_0$.
        \item is coercive if $S(x_n)\to\infty$ whenever $\norm{x_n}\to\infty$.
    \end{enumerate}
\end{definition}

\begin{theorem}
    If the basic assumptions hold. Then $\mathcal{E}_{\alpha,y}:X\to \overline{\real}$ has a minimiser for any $y\in Y$ and $\alpha>0$. 
\end{theorem}
No proof :c, but we have obtained existence. Relies on Banach-Alaoglu theorem. Called Generalised Weierstrass theorem.

\begin{theorem}
    If the basic assumptions hold, $S$ is strictly convex or, $S$ is convex and $A$ is injective. Then $\mathcal{E}_{\alpha,y}$ has a unique minimiser.
\end{theorem}
\begin{remark}
    Makes $R_\alpha$ well-defined.
\end{remark}
We might wonder, is the data-fidelity term convex? The answer is yes, it also strictly convex if and only if it is injective.
\begin{proof}[Proof of convexity of data-fidelity]
    Need to show that for $Q(x)=\frac{1}{2}\norm{Ax-y}^2$, we have
    \begin{equation*}
        Q(\lambda x' + (1-\lambda)x'') < \lambda Q(x') + (1-\lambda)Q(x'')
    \end{equation*} 
    A "lengthy" calculation gives:
    \begin{equation*}
        Q(\lambda x' + (1-\lambda)x'') = \lambda Q(x') + (1-\lambda)Q(x'') - \frac{\lambda(1-\lambda)}{2}\norm{A(x'-x'')}^2
    \end{equation*}
    the final term is clearly positive, so we have convexity. If $A$ is injective $Ax'-x''\neq 0$, since $x'\neq x''$, so strict. If $A$ is not \textcolor{red}{Something is wrong here, the inequality just changed direction}.
\end{proof}


\newpage

\section{Lecture 9 (2025-09-22)}
\textbf{Today}
\begin{itemize}
    \item More theory for variational models
    \item Optimisation methods
\end{itemize}

\subsection{Theory of Variational Models}
\begin{theorem}[Stability]
    Let $R_\alpha:Y\to X$ be defined as $R_\alpha\in \argmin \mathcal{E}_{\alpha,y}(x)$, and that the basic assumptions hold. Also, let $y_n\to y$ in $Y$. Then the following holds
    \begin{enumerate}[label=(\alph*)]
        \item $R_\alpha(y_n)$ converges weakly to $ R_\alpha(y)$ as $n\to\infty$, and
        \item If $S:X\to\overline{\real}$ satisfies Radon-Riesz property saying that, if $x_n$ converges to $x$ weakly and $S(x_n)\to S(x)$, then $x_n$ converges to $x$ strongly. Then $R_\alpha(y_n)$ converges to $R_\alpha(y)$ strongly and $S(R_\alpha(y_n))\to S(R_\alpha(y))$.
    \end{enumerate}
\end{theorem}

The $S$-minmising least square solution is $x^\dagger =\argmin_{x\in L_y} S(x)$, where $L_y= \big\{x\in X: x \text{ minimises } z\mapsto \norm{Az-y}_Y^2\big\}$.

\begin{definition}
    We redefine \textit{regularisation} to mean
    \begin{itemize}
        \item $R_\alpha:Y\to X$ well defined
        \item $R_\alpha$ continuous
        \item $R_\alpha y\to x^\dagger $ when $\alpha\to 0$, and $x^\dagger $ is the $S$-minimising least square solution. 
    \end{itemize}
\end{definition}
Most statements about minimum norm solutions extend to $S$-minimising solution as well. But when do we have a unique $x^\dagger $?

\begin{theorem}
    Assume the basic assumptions hold. And that $y\in AX$. Then we have a unique $S$-minimisig least square solution.
\end{theorem}

\begin{theorem}
    Let $R_\alpha:Y\to X$ be given as in a variational model, and $S:X\to \overline{\real}$ satisfies the basic assumptions. Next, let $y\in AX$, and $y_n\in Y$, and $\delta_n>0$ such that $\norm{y-y_n}\leq \delta_n$ and $\delta_n\to 0$. Finally, we also have an a priori parameter choice rule $\alpha:\real^+\to \real^+$, such that $\alpha(\delta)\to 0$, and $\frac{\delta^2}{\alpha(\delta)}\to 0$ when $\delta \to 0$. Then, 
    \begin{enumerate}[label=(\alph*)]
        \item $x_n=R_{\alpha_n}y_n$, with $\alpha_n = \alpha(\delta_n)$, then $x_n$ converges weakly to $x^\dagger $ as $n\to \infty$, and,
        \item if $S$ satisfies the Radon-Riesz property, then $x_n$ converges strongly to $x^\dagger $.
    \end{enumerate}
\end{theorem}

\subsection{Optimisation methods}
Evaluting $R_\alpha$ for some given $y$ and $\alpha>0$, we need to solve an optimisation problem:
\begin{equation*}
    \argmin_{x\in X} \frac{1}{2}\norm{Ax-y}_Y^2 + \alpha S(x).
\end{equation*}
These problems become very high dimensional. In tomography for example, the images are $512\times 512\times 1000$ (1000 images), i.e. $\real^{512\cdot512\cdot1000}$. Variational models are quite slow, which is why they haven't been used much in medical applications.

Two ways of solving the problem
\begin{enumerate}
    \item Optimise first, then discretise,
    \item Discretise first, then optimise.
\end{enumerate}
In the first version, we formulate the optimisation problem in the infinite dimensional setting, and convergence needs to be proven here. We will be taking the second route however. It is important which inner product we get after discretisation. Just picking the Euclidian norm is not a good plan.


Henceforth, when we say convergence we just mean that $\overline{x}_n\to\overline{x}$ where 
\begin{equation*}
    \overline{x} = \argmin_{x\in X} \norm{Ax-y}_Y^2 + S(x)
\end{equation*}
where $X=\real^N$ with some inner product.

Suppose $E:\real^d\to\real$ is differentiable. We want to find $x_*=\argmin E$. The simplest method is just gradient descent.

\subsubsection{Gradient Descent}
Initialise by taking some $\overline{x}_0\in \real^d$, and compute $\bar{x}_{n+1} = \bar{x}_n - \nu \nabla E(\bar{x}_n)$. If $\bar{x}_*$ is a fixed point, then $\nabla E = 0$. 

The question is, how do we pick $\nu$?

\begin{definition}
    We say that $E:\real^d\to \real$ has $L$-Lipschitz gradient if $E\in C^1(\real^d)$ and $\norm{\nabla E(x') - \nabla E(x'')}\leq L\norm{x'-x''}$.
\end{definition}

\begin{theorem}
    If $0<\nu<1/L$, then $\min_{n=0,1,\dots,N-1} \norm{\nabla E(x_n)}\leq \frac{2}{\nu N}(E(x_0)-\inf_{x\in X} E(x))$, and if $E\in C^2(\real^d)$, then $x_n$ converges to a local minimiser for almost all $x_0$.
\end{theorem}

\newpage

\section{Lecture 10 (2025-09-22)}

\textbf{Today}
\begin{itemize}
    \item Optimisation algorithms
\end{itemize}

\begin{itemize}
    \item Let $E:\real^n\to\overline{\real}$. 
    \item Iterative methods for computing a (local) minimiser to $E$.
    \item Methods
    \begin{itemize}
        \item Second order methods (use the Hessian of $E$)
        \item Quasi-second order methods
        \item First order methods
        \item Stochastic and coordinate-wise first order methods
    \end{itemize}
    \item Order of methods listed from fast to slow convergence, and from bad to good for large scale problems.
\end{itemize}


\subsection{Second order methods}
Assume $E\in C^2(\real^d)$. We will use seconds order information (Hessian), which gives fast convergence. Can incorporate additional constraints via barrier functions. Most classical methods is Newton's methods $x_{k+1} = x_k - \nu (\nabla^2E(x_k))^{-1}\nabla E(x_k)$, clearly expensive each step.

Interior point methods are a big field... All use second order information, and computational bottleneck is that each step needs to solve linear systems. Usually suitable for small and medium-sized problems.


\subsection{Quasi-second order methods}
Here we assume $E\in C^2(\real^d)$, but we only use first order information. Still fairly fast convergence, asymptotically the same, but in practice slightly slower. Best well know are the Quasi-Newton methods $x_{k+1} = x_k - \nu B_k \nabla(x_k)$ where $B_k$ approximated the inverse of the Hessian, but is computed from first order information. The computational bottleneck is now to form the matrix $B_k$. Can sometimes be used for large scale problems. 

\subsection{First order methods}
First order methods are popular for inverse problems. Assume $E\in C^1(\real)$, then we can use \textit{gradient based methods}. If $E$ is not $C^1$, we have to use Proximal methods.

Iterates often cheaper to compute than in (Quasi-)second order methods. Cheaper both in compute power and memory footprint. We only use first order information, which also means slower convergence. 

\subsubsection{Gradient descent}
\begin{itemize}
    \item Suppose $E\in C^1(\real^d)$.
    \item Initialise by picking $x_0$ and $\nu$
    \item Updates $x_{k+1} = x_k - \nu\nabla E(x_k)$
\end{itemize}
If $x_*$ is a critical point to $E$, then $x_*$ is a fixed point to the iterations scheme. But which critical point do we get? 

We would like to know how close to a critical point after a certain number of steps. Assume $E$ has an $L$-Lipschitz gradient, which means that
\begin{equation*}
    \norm{\nabla E(x') + \nabla E(x'')} \leq L \norm{x'-x''}.
\end{equation*}
There are many ways to choose $\nu$ in many ways, we would of course like to choose it to ensure fast convergence. 

Example theorem from the field:
\begin{theorem}
    Let $E\in C^1(\real^d)$ be bounded from below and have $L$-Lipschitz gradient. Then
    \begin{enumerate}[label=(\alph*)]
        \item If $\nu<1/L$, then $\min_{k=0,\dots,n-1} \norm{\nabla E(x_k)}^2\leq \frac{2}{\nu n} (E(x_n)- E_*)$, with $E_* = \inf E$.
        \item If $E\in C^2$, $x_k$ converge to a local minimimum of $E$ for almost all $x_0$.
        \item If $E$ is convex with global minimiser $x_*$, then $E(x_n)-E(x_*)\leq \frac{1}{2\nu n} \norm{x_0-x_*}^2$.
    \end{enumerate}
\end{theorem}
Notice that the convergence rate is $1/n$, which is slow, and a problem in applications. Can we do better without making more assumptions about $E$? Yes, Nesterov acceleration (basis for Adam etc.). 

Possible to improve the convergence rate for general $E$ by modifying the updates. We will only formulate the model. 

\subsubsection{Nesterov's accelerated gradient method}
Initialise $x_0\in\real^d$, $\nu>0, \lambda_0=\beta_0=0$.

Update $z_k = x_k + \beta_k(x_k-x_{k-1}), x_{k+1} = z_k - \nu \nabla E(z_k), \lambda_{k+1} = \frac{1}{2}(1+\sqrt{1+4\lambda_k^2}), \beta_{k+1} = \frac{\lambda_{k}-1}{\lambda_{k+1}}$. Gives convergence rate $1/n^2$, very good.

\begin{theorem}
    If $E\in C^1(\real)$, convex, coercive, and has $L$-Lipschitz gradient, also $\nu<1/L$. Then $x_n$ converges to a local minimiser of $E$ and $E(x_n)- E_* \leq \frac{2 L}{(n+1)^2}\min_{z\in\Omega_E}\norm{z-x_0}^2$, where $E_*=\inf E$, and $\Omega_E = \argmin E(x)$. 
\end{theorem}

Finding $L$ is difficult, often people use adaptive methods. 

\subsubsection{Proximal methods}
Suppose $E$ is not necessarily differentiable. Basis of Proximal methods is non-smooth calculus, which we don't have time for.

The proximal operator (associated with $E$), is defined as 
\begin{equation*}
    \prox_E(x) = \argmin_{z\in\real^d} E(z) + \frac{1}{2}\norm{z-x}^2.
\end{equation*}
Why do we define it like this?
\begin{theorem}
    If $E$ is a closed, proper function. Then $z\mapsto E(z) + \frac{1}{2}\norm{z-x}^2$ is strongly convex, which means $\prox_E:\real^d\to\real^d$ is well-defined.
\end{theorem}
In the problem, we actually want to compute $\prox_{\nu E}$. 

Let $\mathcal{U}\subset \real^d$ be closed and convex, and $E(x) = 0$ for $x\in \mathcal{U}$ and $\infty$ otherwise. Then $\prox_E(x) = \argmin_{z\in \mathcal{U}} \norm{x-z}$. 

Suppose $E\in C^1$, then $\prox_{\nu E} \approx x- \nu \nabla E(x)$, for small $\nu$. If $x_*$ is a fixed point to $\prox_{\nu E}$ if and only if $x_*$ is a minimiser of $E$. 

Suppose $E:\Omega\to\real$, $\Omega\subseteq \real^d$, then $\prox_E$ maps points in $\Omega$ to $\Omega$, moving closer to a minimum of $E$, and points outside $\Omega$ are mapped to $\partial \Omega$. 

Suppose 
\begin{itemize}
    \item $E=G+H$
    \begin{itemize}
        \item $G\in C^1$, with $L$-Lipschitz gradient
        \item $H$ is proper, coercive and l.s.c, but not necessarily differentiable.
    \end{itemize}
    \item Initialise $x_0\in\real^d$ and $\nu<1/L$.
    \item Update $x_{k+1} = \prox_{\nu H}(x_k - \nu \nabla G(x_k))$
\end{itemize}

This is sometimes called forward-backward splitting.
For many examples we know analytic expressions of $\prox$, but not for all, $\norm{x}$ has known $\prox$ but not $\TV$.

\textbf{Douglas-Ratchford}
\begin{itemize}
    \item Let $E=G+H$
    \item Update $z_{k+1} = \frac{1}{2}z_k + \frac{1}{2}(\prox_{\nu G}()$)
\end{itemize}

\textcolor{red}{google it lol}

\end{document}


